# Model arguments
model_name_or_path: /mnt/scratch/home/ismayilz/project-cpo/models/cpo-sft-llama-3.1-8b-instruct-lora-fa-ms30
dataset_name: CNCL-Penn-State/cpo_en_multitask_text_fa_msd5_mm10_ms20_div_nov_sur_qua
dataset_train_split: train
dataset_test_split: val_sample1024
output_dir: /mnt/scratch/home/ismayilz/project-cpo/models/cpo-dpo-sft-ms30-llama-3.1-8b-instruct-lora-fa-msd5-mm10-ms20-cre-exp

# Training arguments
per_device_train_batch_size: 8
per_device_eval_batch_size: 32
learning_rate: 5.0e-6
num_train_epochs: 1
gradient_checkpointing: true
gradient_accumulation_steps: 8
lr_scheduler_type: linear
logging_steps: 16
eval_strategy: steps
eval_steps: 32
save_strategy: steps
save_steps: 512
save_total_limit: 1
max_length: 1024
report_to: wandb
run_name: cpo-dpo-sft-ms30-llama-3.1-8b-instruct-lora-fa-msd5-mm10-ms20-cre-exp
dataloader_num_workers: 4
dataloader_prefetch_factor: 2
seed: 42
remove_unused_columns: false
use_peft: true
lora_r: 128
lora_alpha: 256
fp16: true
lambda_diversity: 1.0
lambda_novelty: 1.0
lambda_surprise: 1.0